{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linghao/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 24%|██▍       | 2391/10000 [00:00<00:00, 23908.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 48%|████▊     | 4809/10000 [00:00<00:00, 23987.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 73%|███████▎  | 7318/10000 [00:00<00:00, 24307.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 10000/10000 [00:00<00:00, 25015.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n ('10.09.70', '1970-09-10'),\n ('4/28/90', '1990-04-28'),\n ('thursday january 26 1995', '1995-01-26'),\n ('monday march 7 1983', '1983-03-07'),\n ('sunday may 22 1988', '1988-05-22'),\n ('tuesday july 8 2008', '2008-07-08'),\n ('08 sep 1999', '1999-09-08'),\n ('1 jan 1981', '1981-01-01'),\n ('monday may 22 1995', '1995-05-22')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\nY.shape: (10000, 10)\nXoh.shape: (10000, 30, 37)\nYoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: 9 may 1998\n",
      "Target date: 1998-05-09\n",
      "\n",
      "Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n",
      "\n",
      "Source after preprocessing (one-hot): [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "Target after preprocessing (one-hot): [[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: one_step_attention\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a,s_prev])\n",
    "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
    "    e = densor1(concat)\n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
    "    energies = densor2(e)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(energies)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas,a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use these layers $T_y$ times in a `for` loop to generate the outputs, and their parameters will not be reinitialized. You will have to carry out the following steps: \n",
    "\n",
    "1. Propagate the input into a [Bidirectional](https://keras.io/layers/wrappers/#bidirectional) [LSTM](https://keras.io/layers/recurrent/#lstm)\n",
    "2. Iterate for $t = 0, \\dots, T_y-1$: \n",
    "    1. Call `one_step_attention()` on $[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$ and $s^{<t-1>}$ to get the context vector $context^{<t>}$.\n",
    "    2. Give $context^{<t>}$ to the post-attention LSTM cell. Remember pass in the previous hidden-state $s^{\\langle t-1\\rangle}$ and cell-states $c^{\\langle t-1\\rangle}$ of this LSTM using `initial_state= [previous hidden state, previous cell state]`. Get back the new hidden state $s^{<t>}$ and the new cell state $c^{<t>}$.\n",
    "    3. Apply a softmax layer to $s^{<t>}$, get the output. \n",
    "    4. Save the output by adding it to the list of outputs.\n",
    "\n",
    "3. Create your Keras model instance, it should have three inputs (\"inputs\", $s^{<0>}$ and $c^{<0>}$) and output the list of \"outputs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(context,initial_state=[s,c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model = Model(inputs=[X,s0,c0],outputs=outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 30, 37)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 30, 64)       17920       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm_6[0][0]                     \n",
      "                                                                 lstm_6[1][0]                     \n",
      "                                                                 lstm_6[2][0]                     \n",
      "                                                                 lstm_6[3][0]                     \n",
      "                                                                 lstm_6[4][0]                     \n",
      "                                                                 lstm_6[5][0]                     \n",
      "                                                                 lstm_6[6][0]                     \n",
      "                                                                 lstm_6[7][0]                     \n",
      "                                                                 lstm_6[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[40][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[41][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[42][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[43][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[44][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[45][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[46][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[47][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[48][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[49][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[40][0]             \n",
      "                                                                 concatenate_1[41][0]             \n",
      "                                                                 concatenate_1[42][0]             \n",
      "                                                                 concatenate_1[43][0]             \n",
      "                                                                 concatenate_1[44][0]             \n",
      "                                                                 concatenate_1[45][0]             \n",
      "                                                                 concatenate_1[46][0]             \n",
      "                                                                 concatenate_1[47][0]             \n",
      "                                                                 concatenate_1[48][0]             \n",
      "                                                                 concatenate_1[49][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 30, 1)        11          dense_1[40][0]                   \n",
      "                                                                 dense_1[41][0]                   \n",
      "                                                                 dense_1[42][0]                   \n",
      "                                                                 dense_1[43][0]                   \n",
      "                                                                 dense_1[44][0]                   \n",
      "                                                                 dense_1[45][0]                   \n",
      "                                                                 dense_1[46][0]                   \n",
      "                                                                 dense_1[47][0]                   \n",
      "                                                                 dense_1[48][0]                   \n",
      "                                                                 dense_1[49][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_2[40][0]                   \n",
      "                                                                 dense_2[41][0]                   \n",
      "                                                                 dense_2[42][0]                   \n",
      "                                                                 dense_2[43][0]                   \n",
      "                                                                 dense_2[44][0]                   \n",
      "                                                                 dense_2[45][0]                   \n",
      "                                                                 dense_2[46][0]                   \n",
      "                                                                 dense_2[47][0]                   \n",
      "                                                                 dense_2[48][0]                   \n",
      "                                                                 dense_2[49][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[40][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[41][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[42][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[43][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[44][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[45][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[46][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[47][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[48][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[49][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 64), (None,  33024       dot_1[40][0]                     \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[41][0]                     \n",
      "                                                                 lstm_6[0][0]                     \n",
      "                                                                 lstm_6[0][2]                     \n",
      "                                                                 dot_1[42][0]                     \n",
      "                                                                 lstm_6[1][0]                     \n",
      "                                                                 lstm_6[1][2]                     \n",
      "                                                                 dot_1[43][0]                     \n",
      "                                                                 lstm_6[2][0]                     \n",
      "                                                                 lstm_6[2][2]                     \n",
      "                                                                 dot_1[44][0]                     \n",
      "                                                                 lstm_6[3][0]                     \n",
      "                                                                 lstm_6[3][2]                     \n",
      "                                                                 dot_1[45][0]                     \n",
      "                                                                 lstm_6[4][0]                     \n",
      "                                                                 lstm_6[4][2]                     \n",
      "                                                                 dot_1[46][0]                     \n",
      "                                                                 lstm_6[5][0]                     \n",
      "                                                                 lstm_6[5][2]                     \n",
      "                                                                 dot_1[47][0]                     \n",
      "                                                                 lstm_6[6][0]                     \n",
      "                                                                 lstm_6[6][2]                     \n",
      "                                                                 dot_1[48][0]                     \n",
      "                                                                 lstm_6[7][0]                     \n",
      "                                                                 lstm_6[7][2]                     \n",
      "                                                                 dot_1[49][0]                     \n",
      "                                                                 lstm_6[8][0]                     \n",
      "                                                                 lstm_6[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 11)           715         lstm_6[0][0]                     \n",
      "                                                                 lstm_6[1][0]                     \n",
      "                                                                 lstm_6[2][0]                     \n",
      "                                                                 lstm_6[3][0]                     \n",
      "                                                                 lstm_6[4][0]                     \n",
      "                                                                 lstm_6[5][0]                     \n",
      "                                                                 lstm_6[6][0]                     \n",
      "                                                                 lstm_6[7][0]                     \n",
      "                                                                 lstm_6[8][0]                     \n",
      "                                                                 lstm_6[9][0]                     \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈2 lines)\n",
    "opt = Adam()\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=opt)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2900/10000 [=======>......................] - ETA: 23:23 - loss: 23.8380 - dense_4_loss: 2.4085 - dense_4_acc: 0.0500 - dense_4_acc_1: 0.0000e+00 - dense_4_acc_2: 0.0000e+00 - dense_4_acc_3: 0.0000e+00 - dense_4_acc_4: 0.9800 - dense_4_acc_5: 0.0100 - dense_4_acc_6: 0.0000e+00 - dense_4_acc_7: 0.9900 - dense_4_acc_8: 0.0200 - dense_4_acc_9: 0.0000e+ - ETA: 11:40 - loss: 23.7859 - dense_4_loss: 2.4049 - dense_4_acc: 0.0500 - dense_4_acc_1: 0.0150 - dense_4_acc_2: 0.0000e+00 - dense_4_acc_3: 0.0050 - dense_4_acc_4: 0.9250 - dense_4_acc_5: 0.0550 - dense_4_acc_6: 0.0100 - dense_4_acc_7: 0.9250 - dense_4_acc_8: 0.0300 - dense_4_acc_9: 0.0200               - ETA: 7:45 - loss: 23.7346 - dense_4_loss: 2.4023 - dense_4_acc: 0.0333 - dense_4_acc_1: 0.0233 - dense_4_acc_2: 0.0133 - dense_4_acc_3: 0.0100 - dense_4_acc_4: 0.8800 - dense_4_acc_5: 0.1000 - dense_4_acc_6: 0.0167 - dense_4_acc_7: 0.8433 - dense_4_acc_8: 0.0433 - dense_4_acc_9: 0.0467     - ETA: 5:48 - loss: 23.6877 - dense_4_loss: 2.4056 - dense_4_acc: 0.0250 - dense_4_acc_1: 0.0475 - dense_4_acc_2: 0.0225 - dense_4_acc_3: 0.0275 - dense_4_acc_4: 0.7825 - dense_4_acc_5: 0.1900 - dense_4_acc_6: 0.0225 - dense_4_acc_7: 0.7350 - dense_4_acc_8: 0.0825 - dense_4_acc_9: 0.050 - ETA: 4:38 - loss: 23.6419 - dense_4_loss: 2.4072 - dense_4_acc: 0.0200 - dense_4_acc_1: 0.0700 - dense_4_acc_2: 0.0360 - dense_4_acc_3: 0.0320 - dense_4_acc_4: 0.7100 - dense_4_acc_5: 0.2440 - dense_4_acc_6: 0.0320 - dense_4_acc_7: 0.6540 - dense_4_acc_8: 0.0980 - dense_4_acc_9: 0.052 - ETA: 3:51 - loss: 23.5987 - dense_4_loss: 2.4131 - dense_4_acc: 0.0167 - dense_4_acc_1: 0.0950 - dense_4_acc_2: 0.0517 - dense_4_acc_3: 0.0367 - dense_4_acc_4: 0.6383 - dense_4_acc_5: 0.2950 - dense_4_acc_6: 0.0300 - dense_4_acc_7: 0.5867 - dense_4_acc_8: 0.1150 - dense_4_acc_9: 0.048 - ETA: 3:17 - loss: 23.5409 - dense_4_loss: 2.4171 - dense_4_acc: 0.0143 - dense_4_acc_1: 0.1214 - dense_4_acc_2: 0.0586 - dense_4_acc_3: 0.0471 - dense_4_acc_4: 0.5843 - dense_4_acc_5: 0.3429 - dense_4_acc_6: 0.0314 - dense_4_acc_7: 0.5371 - dense_4_acc_8: 0.1357 - dense_4_acc_9: 0.050 - ETA: 2:52 - loss: 23.4855 - dense_4_loss: 2.4184 - dense_4_acc: 0.0125 - dense_4_acc_1: 0.1375 - dense_4_acc_2: 0.0725 - dense_4_acc_3: 0.0500 - dense_4_acc_4: 0.5413 - dense_4_acc_5: 0.3725 - dense_4_acc_6: 0.0337 - dense_4_acc_7: 0.5013 - dense_4_acc_8: 0.1487 - dense_4_acc_9: 0.056 - ETA: 2:32 - loss: 23.4280 - dense_4_loss: 2.4257 - dense_4_acc: 0.0111 - dense_4_acc_1: 0.1544 - dense_4_acc_2: 0.0867 - dense_4_acc_3: 0.0489 - dense_4_acc_4: 0.5033 - dense_4_acc_5: 0.3967 - dense_4_acc_6: 0.0367 - dense_4_acc_7: 0.4678 - dense_4_acc_8: 0.1578 - dense_4_acc_9: 0.057 - ETA: 2:16 - loss: 23.3652 - dense_4_loss: 2.4333 - dense_4_acc: 0.0100 - dense_4_acc_1: 0.1740 - dense_4_acc_2: 0.0990 - dense_4_acc_3: 0.0520 - dense_4_acc_4: 0.4720 - dense_4_acc_5: 0.4100 - dense_4_acc_6: 0.0390 - dense_4_acc_7: 0.4470 - dense_4_acc_8: 0.1600 - dense_4_acc_9: 0.060 - ETA: 2:04 - loss: 23.2982 - dense_4_loss: 2.4414 - dense_4_acc: 0.0091 - dense_4_acc_1: 0.1864 - dense_4_acc_2: 0.1018 - dense_4_acc_3: 0.0545 - dense_4_acc_4: 0.4555 - dense_4_acc_5: 0.4127 - dense_4_acc_6: 0.0391 - dense_4_acc_7: 0.4636 - dense_4_acc_8: 0.1555 - dense_4_acc_9: 0.059 - ETA: 1:53 - loss: 23.2333 - dense_4_loss: 2.4494 - dense_4_acc: 0.0083 - dense_4_acc_1: 0.1967 - dense_4_acc_2: 0.1042 - dense_4_acc_3: 0.0542 - dense_4_acc_4: 0.4733 - dense_4_acc_5: 0.3933 - dense_4_acc_6: 0.0358 - dense_4_acc_7: 0.5025 - dense_4_acc_8: 0.1425 - dense_4_acc_9: 0.054 - ETA: 1:44 - loss: 23.1744 - dense_4_loss: 2.4660 - dense_4_acc: 0.0077 - dense_4_acc_1: 0.2031 - dense_4_acc_2: 0.1054 - dense_4_acc_3: 0.0538 - dense_4_acc_4: 0.5092 - dense_4_acc_5: 0.3638 - dense_4_acc_6: 0.0331 - dense_4_acc_7: 0.5408 - dense_4_acc_8: 0.1315 - dense_4_acc_9: 0.050 - ETA: 1:36 - loss: 23.1038 - dense_4_loss: 2.4831 - dense_4_acc: 0.0071 - dense_4_acc_1: 0.2129 - dense_4_acc_2: 0.1079 - dense_4_acc_3: 0.0500 - dense_4_acc_4: 0.5443 - dense_4_acc_5: 0.3379 - dense_4_acc_6: 0.0307 - dense_4_acc_7: 0.5736 - dense_4_acc_8: 0.1221 - dense_4_acc_9: 0.046 - ETA: 1:29 - loss: 23.0293 - dense_4_loss: 2.5032 - dense_4_acc: 0.0067 - dense_4_acc_1: 0.2140 - dense_4_acc_2: 0.1007 - dense_4_acc_3: 0.0467 - dense_4_acc_4: 0.5747 - dense_4_acc_5: 0.3153 - dense_4_acc_6: 0.0287 - dense_4_acc_7: 0.6020 - dense_4_acc_8: 0.1140 - dense_4_acc_9: 0.043 - ETA: 1:23 - loss: 22.9651 - dense_4_loss: 2.5289 - dense_4_acc: 0.0063 - dense_4_acc_1: 0.2081 - dense_4_acc_2: 0.0944 - dense_4_acc_3: 0.0437 - dense_4_acc_4: 0.6013 - dense_4_acc_5: 0.2956 - dense_4_acc_6: 0.0269 - dense_4_acc_7: 0.6269 - dense_4_acc_8: 0.1069 - dense_4_acc_9: 0.040 - ETA: 1:18 - loss: 22.9087 - dense_4_loss: 2.5702 - dense_4_acc: 0.0059 - dense_4_acc_1: 0.1988 - dense_4_acc_2: 0.0888 - dense_4_acc_3: 0.0412 - dense_4_acc_4: 0.6247 - dense_4_acc_5: 0.2782 - dense_4_acc_6: 0.0253 - dense_4_acc_7: 0.6488 - dense_4_acc_8: 0.1006 - dense_4_acc_9: 0.038 - ETA: 1:13 - loss: 22.8521 - dense_4_loss: 2.6004 - dense_4_acc: 0.0056 - dense_4_acc_1: 0.1883 - dense_4_acc_2: 0.0839 - dense_4_acc_3: 0.0389 - dense_4_acc_4: 0.6456 - dense_4_acc_5: 0.2628 - dense_4_acc_6: 0.0239 - dense_4_acc_7: 0.6683 - dense_4_acc_8: 0.0950 - dense_4_acc_9: 0.036 - ETA: 1:09 - loss: 22.8117 - dense_4_loss: 2.6267 - dense_4_acc: 0.0053 - dense_4_acc_1: 0.1789 - dense_4_acc_2: 0.0795 - dense_4_acc_3: 0.0368 - dense_4_acc_4: 0.6642 - dense_4_acc_5: 0.2489 - dense_4_acc_6: 0.0226 - dense_4_acc_7: 0.6858 - dense_4_acc_8: 0.0900 - dense_4_acc_9: 0.034 - ETA: 1:05 - loss: 22.7720 - dense_4_loss: 2.6511 - dense_4_acc: 0.0050 - dense_4_acc_1: 0.1720 - dense_4_acc_2: 0.0755 - dense_4_acc_3: 0.0350 - dense_4_acc_4: 0.6810 - dense_4_acc_5: 0.2365 - dense_4_acc_6: 0.0215 - dense_4_acc_7: 0.7015 - dense_4_acc_8: 0.0855 - dense_4_acc_9: 0.032 - ETA: 1:02 - loss: 22.7372 - dense_4_loss: 2.6787 - dense_4_acc: 0.0048 - dense_4_acc_1: 0.1643 - dense_4_acc_2: 0.0719 - dense_4_acc_3: 0.0333 - dense_4_acc_4: 0.6962 - dense_4_acc_5: 0.2252 - dense_4_acc_6: 0.0205 - dense_4_acc_7: 0.7157 - dense_4_acc_8: 0.0814 - dense_4_acc_9: 0.031 - ETA: 58s - loss: 22.6926 - dense_4_loss: 2.6940 - dense_4_acc: 0.0045 - dense_4_acc_1: 0.1586 - dense_4_acc_2: 0.0686 - dense_4_acc_3: 0.0318 - dense_4_acc_4: 0.7100 - dense_4_acc_5: 0.2150 - dense_4_acc_6: 0.0195 - dense_4_acc_7: 0.7286 - dense_4_acc_8: 0.0777 - dense_4_acc_9: 0.029 - ETA: 56s - loss: 22.6580 - dense_4_loss: 2.7107 - dense_4_acc: 0.0043 - dense_4_acc_1: 0.1565 - dense_4_acc_2: 0.0657 - dense_4_acc_3: 0.0304 - dense_4_acc_4: 0.7226 - dense_4_acc_5: 0.2057 - dense_4_acc_6: 0.0187 - dense_4_acc_7: 0.7404 - dense_4_acc_8: 0.0743 - dense_4_acc_9: 0.02 - ETA: 53s - loss: 22.6243 - dense_4_loss: 2.7254 - dense_4_acc: 0.0042 - dense_4_acc_1: 0.1571 - dense_4_acc_2: 0.0629 - dense_4_acc_3: 0.0292 - dense_4_acc_4: 0.7342 - dense_4_acc_5: 0.1971 - dense_4_acc_6: 0.0179 - dense_4_acc_7: 0.7513 - dense_4_acc_8: 0.0712 - dense_4_acc_9: 0.02 - ETA: 50s - loss: 22.5969 - dense_4_loss: 2.7419 - dense_4_acc: 0.0040 - dense_4_acc_1: 0.1596 - dense_4_acc_2: 0.0604 - dense_4_acc_3: 0.0280 - dense_4_acc_4: 0.7448 - dense_4_acc_5: 0.1892 - dense_4_acc_6: 0.0172 - dense_4_acc_7: 0.7612 - dense_4_acc_8: 0.0684 - dense_4_acc_9: 0.02 - ETA: 48s - loss: 22.5587 - dense_4_loss: 2.7466 - dense_4_acc: 0.0038 - dense_4_acc_1: 0.1665 - dense_4_acc_2: 0.0581 - dense_4_acc_3: 0.0269 - dense_4_acc_4: 0.7546 - dense_4_acc_5: 0.1819 - dense_4_acc_6: 0.0165 - dense_4_acc_7: 0.7704 - dense_4_acc_8: 0.0658 - dense_4_acc_9: 0.02 - ETA: 46s - loss: 22.5260 - dense_4_loss: 2.7542 - dense_4_acc: 0.0037 - dense_4_acc_1: 0.1707 - dense_4_acc_2: 0.0559 - dense_4_acc_3: 0.0259 - dense_4_acc_4: 0.7637 - dense_4_acc_5: 0.1752 - dense_4_acc_6: 0.0159 - dense_4_acc_7: 0.7789 - dense_4_acc_8: 0.0633 - dense_4_acc_9: 0.02 - ETA: 44s - loss: 22.4982 - dense_4_loss: 2.7630 - dense_4_acc: 0.0036 - dense_4_acc_1: 0.1793 - dense_4_acc_2: 0.0539 - dense_4_acc_3: 0.0250 - dense_4_acc_4: 0.7721 - dense_4_acc_5: 0.1689 - dense_4_acc_6: 0.0154 - dense_4_acc_7: 0.7868 - dense_4_acc_8: 0.0611 - dense_4_acc_9: 0.02 - ETA: 42s - loss: 22.4626 - dense_4_loss: 2.7646 - dense_4_acc: 0.0034 - dense_4_acc_1: 0.1869 - dense_4_acc_2: 0.0521 - dense_4_acc_3: 0.0241 - dense_4_acc_4: 0.7800 - dense_4_acc_5: 0.1631 - dense_4_acc_6: 0.0148 - dense_4_acc_7: 0.7941 - dense_4_acc_8: 0.0590 - dense_4_acc_9: 0.02 5800/10000 [================>.............] - ETA: 40s - loss: 22.4249 - dense_4_loss: 2.7642 - dense_4_acc: 0.0033 - dense_4_acc_1: 0.1923 - dense_4_acc_2: 0.0503 - dense_4_acc_3: 0.0233 - dense_4_acc_4: 0.7873 - dense_4_acc_5: 0.1577 - dense_4_acc_6: 0.0143 - dense_4_acc_7: 0.8010 - dense_4_acc_8: 0.0570 - dense_4_acc_9: 0.02 - ETA: 39s - loss: 22.3963 - dense_4_loss: 2.7671 - dense_4_acc: 0.0032 - dense_4_acc_1: 0.1981 - dense_4_acc_2: 0.0487 - dense_4_acc_3: 0.0226 - dense_4_acc_4: 0.7942 - dense_4_acc_5: 0.1526 - dense_4_acc_6: 0.0139 - dense_4_acc_7: 0.8074 - dense_4_acc_8: 0.0552 - dense_4_acc_9: 0.02 - ETA: 37s - loss: 22.3730 - dense_4_loss: 2.7695 - dense_4_acc: 0.0031 - dense_4_acc_1: 0.2019 - dense_4_acc_2: 0.0472 - dense_4_acc_3: 0.0219 - dense_4_acc_4: 0.8006 - dense_4_acc_5: 0.1478 - dense_4_acc_6: 0.0134 - dense_4_acc_7: 0.8134 - dense_4_acc_8: 0.0534 - dense_4_acc_9: 0.02 - ETA: 36s - loss: 22.3445 - dense_4_loss: 2.7715 - dense_4_acc: 0.0030 - dense_4_acc_1: 0.2079 - dense_4_acc_2: 0.0458 - dense_4_acc_3: 0.0212 - dense_4_acc_4: 0.8067 - dense_4_acc_5: 0.1433 - dense_4_acc_6: 0.0130 - dense_4_acc_7: 0.8191 - dense_4_acc_8: 0.0518 - dense_4_acc_9: 0.01 - ETA: 34s - loss: 22.3167 - dense_4_loss: 2.7720 - dense_4_acc: 0.0029 - dense_4_acc_1: 0.2132 - dense_4_acc_2: 0.0453 - dense_4_acc_3: 0.0206 - dense_4_acc_4: 0.8124 - dense_4_acc_5: 0.1391 - dense_4_acc_6: 0.0126 - dense_4_acc_7: 0.8244 - dense_4_acc_8: 0.0503 - dense_4_acc_9: 0.01 - ETA: 33s - loss: 22.2908 - dense_4_loss: 2.7753 - dense_4_acc: 0.0029 - dense_4_acc_1: 0.2189 - dense_4_acc_2: 0.0466 - dense_4_acc_3: 0.0200 - dense_4_acc_4: 0.8177 - dense_4_acc_5: 0.1351 - dense_4_acc_6: 0.0123 - dense_4_acc_7: 0.8294 - dense_4_acc_8: 0.0489 - dense_4_acc_9: 0.01 - ETA: 32s - loss: 22.2694 - dense_4_loss: 2.7810 - dense_4_acc: 0.0028 - dense_4_acc_1: 0.2228 - dense_4_acc_2: 0.0500 - dense_4_acc_3: 0.0194 - dense_4_acc_4: 0.8228 - dense_4_acc_5: 0.1314 - dense_4_acc_6: 0.0119 - dense_4_acc_7: 0.8342 - dense_4_acc_8: 0.0475 - dense_4_acc_9: 0.01 - ETA: 31s - loss: 22.2474 - dense_4_loss: 2.7838 - dense_4_acc: 0.0027 - dense_4_acc_1: 0.2249 - dense_4_acc_2: 0.0519 - dense_4_acc_3: 0.0189 - dense_4_acc_4: 0.8276 - dense_4_acc_5: 0.1278 - dense_4_acc_6: 0.0116 - dense_4_acc_7: 0.8386 - dense_4_acc_8: 0.0462 - dense_4_acc_9: 0.01 - ETA: 30s - loss: 22.2234 - dense_4_loss: 2.7881 - dense_4_acc: 0.0026 - dense_4_acc_1: 0.2297 - dense_4_acc_2: 0.0550 - dense_4_acc_3: 0.0184 - dense_4_acc_4: 0.8321 - dense_4_acc_5: 0.1245 - dense_4_acc_6: 0.0113 - dense_4_acc_7: 0.8429 - dense_4_acc_8: 0.0450 - dense_4_acc_9: 0.01 - ETA: 29s - loss: 22.2003 - dense_4_loss: 2.7898 - dense_4_acc: 0.0026 - dense_4_acc_1: 0.2338 - dense_4_acc_2: 0.0587 - dense_4_acc_3: 0.0179 - dense_4_acc_4: 0.8364 - dense_4_acc_5: 0.1213 - dense_4_acc_6: 0.0110 - dense_4_acc_7: 0.8469 - dense_4_acc_8: 0.0438 - dense_4_acc_9: 0.01 - ETA: 28s - loss: 22.1768 - dense_4_loss: 2.7937 - dense_4_acc: 0.0025 - dense_4_acc_1: 0.2387 - dense_4_acc_2: 0.0642 - dense_4_acc_3: 0.0175 - dense_4_acc_4: 0.8405 - dense_4_acc_5: 0.1182 - dense_4_acc_6: 0.0107 - dense_4_acc_7: 0.8508 - dense_4_acc_8: 0.0427 - dense_4_acc_9: 0.01 - ETA: 27s - loss: 22.1538 - dense_4_loss: 2.7984 - dense_4_acc: 0.0024 - dense_4_acc_1: 0.2429 - dense_4_acc_2: 0.0680 - dense_4_acc_3: 0.0171 - dense_4_acc_4: 0.8444 - dense_4_acc_5: 0.1154 - dense_4_acc_6: 0.0105 - dense_4_acc_7: 0.8544 - dense_4_acc_8: 0.0417 - dense_4_acc_9: 0.01 - ETA: 26s - loss: 22.1330 - dense_4_loss: 2.8014 - dense_4_acc: 0.0024 - dense_4_acc_1: 0.2460 - dense_4_acc_2: 0.0714 - dense_4_acc_3: 0.0167 - dense_4_acc_4: 0.8481 - dense_4_acc_5: 0.1126 - dense_4_acc_6: 0.0102 - dense_4_acc_7: 0.8579 - dense_4_acc_8: 0.0407 - dense_4_acc_9: 0.01 - ETA: 25s - loss: 22.1106 - dense_4_loss: 2.8035 - dense_4_acc: 0.0023 - dense_4_acc_1: 0.2477 - dense_4_acc_2: 0.0740 - dense_4_acc_3: 0.0174 - dense_4_acc_4: 0.8516 - dense_4_acc_5: 0.1100 - dense_4_acc_6: 0.0100 - dense_4_acc_7: 0.8612 - dense_4_acc_8: 0.0398 - dense_4_acc_9: 0.01 - ETA: 24s - loss: 22.0936 - dense_4_loss: 2.8064 - dense_4_acc: 0.0023 - dense_4_acc_1: 0.2505 - dense_4_acc_2: 0.0775 - dense_4_acc_3: 0.0177 - dense_4_acc_4: 0.8550 - dense_4_acc_5: 0.1075 - dense_4_acc_6: 0.0098 - dense_4_acc_7: 0.8643 - dense_4_acc_8: 0.0389 - dense_4_acc_9: 0.01 - ETA: 23s - loss: 22.0751 - dense_4_loss: 2.8039 - dense_4_acc: 0.0022 - dense_4_acc_1: 0.2518 - dense_4_acc_2: 0.0780 - dense_4_acc_3: 0.0173 - dense_4_acc_4: 0.8582 - dense_4_acc_5: 0.1051 - dense_4_acc_6: 0.0096 - dense_4_acc_7: 0.8673 - dense_4_acc_8: 0.0380 - dense_4_acc_9: 0.01 - ETA: 22s - loss: 22.0527 - dense_4_loss: 2.8022 - dense_4_acc: 0.0022 - dense_4_acc_1: 0.2546 - dense_4_acc_2: 0.0800 - dense_4_acc_3: 0.0187 - dense_4_acc_4: 0.8613 - dense_4_acc_5: 0.1028 - dense_4_acc_6: 0.0093 - dense_4_acc_7: 0.8702 - dense_4_acc_8: 0.0372 - dense_4_acc_9: 0.01 - ETA: 22s - loss: 22.0317 - dense_4_loss: 2.8057 - dense_4_acc: 0.0021 - dense_4_acc_1: 0.2574 - dense_4_acc_2: 0.0826 - dense_4_acc_3: 0.0198 - dense_4_acc_4: 0.8643 - dense_4_acc_5: 0.1006 - dense_4_acc_6: 0.0091 - dense_4_acc_7: 0.8730 - dense_4_acc_8: 0.0364 - dense_4_acc_9: 0.01 - ETA: 21s - loss: 22.0079 - dense_4_loss: 2.8051 - dense_4_acc: 0.0021 - dense_4_acc_1: 0.2587 - dense_4_acc_2: 0.0838 - dense_4_acc_3: 0.0215 - dense_4_acc_4: 0.8671 - dense_4_acc_5: 0.0985 - dense_4_acc_6: 0.0090 - dense_4_acc_7: 0.8756 - dense_4_acc_8: 0.0356 - dense_4_acc_9: 0.01 - ETA: 20s - loss: 21.9862 - dense_4_loss: 2.8047 - dense_4_acc: 0.0020 - dense_4_acc_1: 0.2600 - dense_4_acc_2: 0.0859 - dense_4_acc_3: 0.0239 - dense_4_acc_4: 0.8698 - dense_4_acc_5: 0.0965 - dense_4_acc_6: 0.0088 - dense_4_acc_7: 0.8782 - dense_4_acc_8: 0.0349 - dense_4_acc_9: 0.01 - ETA: 19s - loss: 21.9630 - dense_4_loss: 2.8047 - dense_4_acc: 0.0020 - dense_4_acc_1: 0.2630 - dense_4_acc_2: 0.0886 - dense_4_acc_3: 0.0246 - dense_4_acc_4: 0.8724 - dense_4_acc_5: 0.0946 - dense_4_acc_6: 0.0086 - dense_4_acc_7: 0.8806 - dense_4_acc_8: 0.0342 - dense_4_acc_9: 0.01 - ETA: 19s - loss: 21.9394 - dense_4_loss: 2.8037 - dense_4_acc: 0.0020 - dense_4_acc_1: 0.2655 - dense_4_acc_2: 0.0910 - dense_4_acc_3: 0.0269 - dense_4_acc_4: 0.8749 - dense_4_acc_5: 0.0927 - dense_4_acc_6: 0.0084 - dense_4_acc_7: 0.8829 - dense_4_acc_8: 0.0335 - dense_4_acc_9: 0.01 - ETA: 18s - loss: 21.9149 - dense_4_loss: 2.8045 - dense_4_acc: 0.0019 - dense_4_acc_1: 0.2685 - dense_4_acc_2: 0.0944 - dense_4_acc_3: 0.0283 - dense_4_acc_4: 0.8773 - dense_4_acc_5: 0.0910 - dense_4_acc_6: 0.0083 - dense_4_acc_7: 0.8852 - dense_4_acc_8: 0.0329 - dense_4_acc_9: 0.01 - ETA: 17s - loss: 21.8910 - dense_4_loss: 2.8036 - dense_4_acc: 0.0019 - dense_4_acc_1: 0.2700 - dense_4_acc_2: 0.0958 - dense_4_acc_3: 0.0300 - dense_4_acc_4: 0.8796 - dense_4_acc_5: 0.0892 - dense_4_acc_6: 0.0081 - dense_4_acc_7: 0.8874 - dense_4_acc_8: 0.0323 - dense_4_acc_9: 0.01 - ETA: 17s - loss: 21.8677 - dense_4_loss: 2.8028 - dense_4_acc: 0.0019 - dense_4_acc_1: 0.2720 - dense_4_acc_2: 0.0981 - dense_4_acc_3: 0.0315 - dense_4_acc_4: 0.8819 - dense_4_acc_5: 0.0876 - dense_4_acc_6: 0.0080 - dense_4_acc_7: 0.8894 - dense_4_acc_8: 0.0317 - dense_4_acc_9: 0.01 - ETA: 16s - loss: 21.8396 - dense_4_loss: 2.8018 - dense_4_acc: 0.0018 - dense_4_acc_1: 0.2742 - dense_4_acc_2: 0.0995 - dense_4_acc_3: 0.0331 - dense_4_acc_4: 0.8840 - dense_4_acc_5: 0.0860 - dense_4_acc_6: 0.0078 - dense_4_acc_7: 0.8915 - dense_4_acc_8: 0.0311 - dense_4_acc_9: 0.01 - ETA: 16s - loss: 21.8136 - dense_4_loss: 2.8003 - dense_4_acc: 0.0018 - dense_4_acc_1: 0.2827 - dense_4_acc_2: 0.1016 - dense_4_acc_3: 0.0339 - dense_4_acc_4: 0.8861 - dense_4_acc_5: 0.0845 - dense_4_acc_6: 0.0077 - dense_4_acc_7: 0.8934 - dense_4_acc_8: 0.0305 - dense_4_acc_9: 0.01 - ETA: 15s - loss: 21.7918 - dense_4_loss: 2.7986 - dense_4_acc: 0.0018 - dense_4_acc_1: 0.2882 - dense_4_acc_2: 0.1035 - dense_4_acc_3: 0.0342 - dense_4_acc_4: 0.8881 - dense_4_acc_5: 0.0830 - dense_4_acc_6: 0.0075 - dense_4_acc_7: 0.8953 - dense_4_acc_8: 0.0300 - dense_4_acc_9: 0.01 - ETA: 15s - loss: 21.7670 - dense_4_loss: 2.7984 - dense_4_acc: 0.0017 - dense_4_acc_1: 0.2938 - dense_4_acc_2: 0.1057 - dense_4_acc_3: 0.0357 - dense_4_acc_4: 0.8900 - dense_4_acc_5: 0.0816 - dense_4_acc_6: 0.0074 - dense_4_acc_7: 0.8971 - dense_4_acc_8: 0.0295 - dense_4_acc_9: 0.0129 8700/10000 [=========================>....] - ETA: 14s - loss: 21.7435 - dense_4_loss: 2.7968 - dense_4_acc: 0.0017 - dense_4_acc_1: 0.3003 - dense_4_acc_2: 0.1075 - dense_4_acc_3: 0.0361 - dense_4_acc_4: 0.8919 - dense_4_acc_5: 0.0802 - dense_4_acc_6: 0.0073 - dense_4_acc_7: 0.8988 - dense_4_acc_8: 0.0292 - dense_4_acc_9: 0.01 - ETA: 14s - loss: 21.7145 - dense_4_loss: 2.7962 - dense_4_acc: 0.0017 - dense_4_acc_1: 0.3072 - dense_4_acc_2: 0.1087 - dense_4_acc_3: 0.0377 - dense_4_acc_4: 0.8937 - dense_4_acc_5: 0.0788 - dense_4_acc_6: 0.0072 - dense_4_acc_7: 0.9005 - dense_4_acc_8: 0.0300 - dense_4_acc_9: 0.01 - ETA: 13s - loss: 21.6862 - dense_4_loss: 2.7949 - dense_4_acc: 0.0016 - dense_4_acc_1: 0.3133 - dense_4_acc_2: 0.1121 - dense_4_acc_3: 0.0392 - dense_4_acc_4: 0.8954 - dense_4_acc_5: 0.0775 - dense_4_acc_6: 0.0070 - dense_4_acc_7: 0.9021 - dense_4_acc_8: 0.0297 - dense_4_acc_9: 0.01 - ETA: 13s - loss: 21.6594 - dense_4_loss: 2.7931 - dense_4_acc: 0.0016 - dense_4_acc_1: 0.3184 - dense_4_acc_2: 0.1140 - dense_4_acc_3: 0.0406 - dense_4_acc_4: 0.8971 - dense_4_acc_5: 0.0763 - dense_4_acc_6: 0.0069 - dense_4_acc_7: 0.9037 - dense_4_acc_8: 0.0292 - dense_4_acc_9: 0.01 - ETA: 12s - loss: 21.6365 - dense_4_loss: 2.7925 - dense_4_acc: 0.0016 - dense_4_acc_1: 0.3230 - dense_4_acc_2: 0.1137 - dense_4_acc_3: 0.0422 - dense_4_acc_4: 0.8987 - dense_4_acc_5: 0.0751 - dense_4_acc_6: 0.0068 - dense_4_acc_7: 0.9052 - dense_4_acc_8: 0.0289 - dense_4_acc_9: 0.02 - ETA: 12s - loss: 21.6104 - dense_4_loss: 2.7902 - dense_4_acc: 0.0016 - dense_4_acc_1: 0.3277 - dense_4_acc_2: 0.1158 - dense_4_acc_3: 0.0433 - dense_4_acc_4: 0.9003 - dense_4_acc_5: 0.0739 - dense_4_acc_6: 0.0067 - dense_4_acc_7: 0.9067 - dense_4_acc_8: 0.0298 - dense_4_acc_9: 0.02 - ETA: 11s - loss: 21.5863 - dense_4_loss: 2.7878 - dense_4_acc: 0.0015 - dense_4_acc_1: 0.3306 - dense_4_acc_2: 0.1163 - dense_4_acc_3: 0.0442 - dense_4_acc_4: 0.9018 - dense_4_acc_5: 0.0728 - dense_4_acc_6: 0.0066 - dense_4_acc_7: 0.9082 - dense_4_acc_8: 0.0343 - dense_4_acc_9: 0.02 - ETA: 11s - loss: 21.5612 - dense_4_loss: 2.7853 - dense_4_acc: 0.0015 - dense_4_acc_1: 0.3333 - dense_4_acc_2: 0.1180 - dense_4_acc_3: 0.0458 - dense_4_acc_4: 0.9033 - dense_4_acc_5: 0.0717 - dense_4_acc_6: 0.0065 - dense_4_acc_7: 0.9095 - dense_4_acc_8: 0.0408 - dense_4_acc_9: 0.02 - ETA: 10s - loss: 21.5378 - dense_4_loss: 2.7831 - dense_4_acc: 0.0015 - dense_4_acc_1: 0.3361 - dense_4_acc_2: 0.1201 - dense_4_acc_3: 0.0464 - dense_4_acc_4: 0.9048 - dense_4_acc_5: 0.0706 - dense_4_acc_6: 0.0064 - dense_4_acc_7: 0.9109 - dense_4_acc_8: 0.0457 - dense_4_acc_9: 0.02 - ETA: 10s - loss: 21.5105 - dense_4_loss: 2.7815 - dense_4_acc: 0.0015 - dense_4_acc_1: 0.3403 - dense_4_acc_2: 0.1219 - dense_4_acc_3: 0.0474 - dense_4_acc_4: 0.9062 - dense_4_acc_5: 0.0696 - dense_4_acc_6: 0.0063 - dense_4_acc_7: 0.9122 - dense_4_acc_8: 0.0490 - dense_4_acc_9: 0.02 - ETA: 9s - loss: 21.4832 - dense_4_loss: 2.7809 - dense_4_acc: 0.0014 - dense_4_acc_1: 0.3445 - dense_4_acc_2: 0.1232 - dense_4_acc_3: 0.0480 - dense_4_acc_4: 0.9075 - dense_4_acc_5: 0.0686 - dense_4_acc_6: 0.0062 - dense_4_acc_7: 0.9135 - dense_4_acc_8: 0.0483 - dense_4_acc_9: 0.0290 - ETA: 9s - loss: 21.4567 - dense_4_loss: 2.7771 - dense_4_acc: 0.0014 - dense_4_acc_1: 0.3474 - dense_4_acc_2: 0.1249 - dense_4_acc_3: 0.0484 - dense_4_acc_4: 0.9089 - dense_4_acc_5: 0.0676 - dense_4_acc_6: 0.0061 - dense_4_acc_7: 0.9147 - dense_4_acc_8: 0.0476 - dense_4_acc_9: 0.030 - ETA: 9s - loss: 21.4301 - dense_4_loss: 2.7747 - dense_4_acc: 0.0014 - dense_4_acc_1: 0.3508 - dense_4_acc_2: 0.1282 - dense_4_acc_3: 0.0494 - dense_4_acc_4: 0.9101 - dense_4_acc_5: 0.0666 - dense_4_acc_6: 0.0061 - dense_4_acc_7: 0.9159 - dense_4_acc_8: 0.0494 - dense_4_acc_9: 0.031 - ETA: 8s - loss: 21.4036 - dense_4_loss: 2.7734 - dense_4_acc: 0.0014 - dense_4_acc_1: 0.3542 - dense_4_acc_2: 0.1308 - dense_4_acc_3: 0.0507 - dense_4_acc_4: 0.9114 - dense_4_acc_5: 0.0657 - dense_4_acc_6: 0.0060 - dense_4_acc_7: 0.9171 - dense_4_acc_8: 0.0529 - dense_4_acc_9: 0.032 - ETA: 8s - loss: 21.3792 - dense_4_loss: 2.7725 - dense_4_acc: 0.0014 - dense_4_acc_1: 0.3574 - dense_4_acc_2: 0.1321 - dense_4_acc_3: 0.0514 - dense_4_acc_4: 0.9126 - dense_4_acc_5: 0.0648 - dense_4_acc_6: 0.0059 - dense_4_acc_7: 0.9182 - dense_4_acc_8: 0.0575 - dense_4_acc_9: 0.033 - ETA: 7s - loss: 21.3544 - dense_4_loss: 2.7708 - dense_4_acc: 0.0014 - dense_4_acc_1: 0.3611 - dense_4_acc_2: 0.1330 - dense_4_acc_3: 0.0516 - dense_4_acc_4: 0.9138 - dense_4_acc_5: 0.0639 - dense_4_acc_6: 0.0058 - dense_4_acc_7: 0.9193 - dense_4_acc_8: 0.0572 - dense_4_acc_9: 0.035 - ETA: 7s - loss: 21.3264 - dense_4_loss: 2.7685 - dense_4_acc: 0.0013 - dense_4_acc_1: 0.3656 - dense_4_acc_2: 0.1339 - dense_4_acc_3: 0.0516 - dense_4_acc_4: 0.9149 - dense_4_acc_5: 0.0631 - dense_4_acc_6: 0.0057 - dense_4_acc_7: 0.9204 - dense_4_acc_8: 0.0564 - dense_4_acc_9: 0.036 - ETA: 7s - loss: 21.2995 - dense_4_loss: 2.7663 - dense_4_acc: 0.0013 - dense_4_acc_1: 0.3692 - dense_4_acc_2: 0.1355 - dense_4_acc_3: 0.0521 - dense_4_acc_4: 0.9161 - dense_4_acc_5: 0.0622 - dense_4_acc_6: 0.0057 - dense_4_acc_7: 0.9214 - dense_4_acc_8: 0.0557 - dense_4_acc_9: 0.037 - ETA: 6s - loss: 21.2736 - dense_4_loss: 2.7636 - dense_4_acc: 0.0013 - dense_4_acc_1: 0.3718 - dense_4_acc_2: 0.1357 - dense_4_acc_3: 0.0522 - dense_4_acc_4: 0.9171 - dense_4_acc_5: 0.0614 - dense_4_acc_6: 0.0056 - dense_4_acc_7: 0.9225 - dense_4_acc_8: 0.0574 - dense_4_acc_9: 0.038 - ETA: 6s - loss: 21.2475 - dense_4_loss: 2.7630 - dense_4_acc: 0.0013 - dense_4_acc_1: 0.3753 - dense_4_acc_2: 0.1376 - dense_4_acc_3: 0.0523 - dense_4_acc_4: 0.9182 - dense_4_acc_5: 0.0606 - dense_4_acc_6: 0.0055 - dense_4_acc_7: 0.9235 - dense_4_acc_8: 0.0624 - dense_4_acc_9: 0.038 - ETA: 6s - loss: 21.2205 - dense_4_loss: 2.7618 - dense_4_acc: 0.0013 - dense_4_acc_1: 0.3786 - dense_4_acc_2: 0.1392 - dense_4_acc_3: 0.0523 - dense_4_acc_4: 0.9192 - dense_4_acc_5: 0.0599 - dense_4_acc_6: 0.0054 - dense_4_acc_7: 0.9244 - dense_4_acc_8: 0.0673 - dense_4_acc_9: 0.039 - ETA: 5s - loss: 21.1943 - dense_4_loss: 2.7596 - dense_4_acc: 0.0013 - dense_4_acc_1: 0.3819 - dense_4_acc_2: 0.1400 - dense_4_acc_3: 0.0524 - dense_4_acc_4: 0.9203 - dense_4_acc_5: 0.0591 - dense_4_acc_6: 0.0054 - dense_4_acc_7: 0.9254 - dense_4_acc_8: 0.0686 - dense_4_acc_9: 0.040 - ETA: 5s - loss: 21.1669 - dense_4_loss: 2.7567 - dense_4_acc: 0.0012 - dense_4_acc_1: 0.3846 - dense_4_acc_2: 0.1411 - dense_4_acc_3: 0.0525 - dense_4_acc_4: 0.9212 - dense_4_acc_5: 0.0584 - dense_4_acc_6: 0.0053 - dense_4_acc_7: 0.9263 - dense_4_acc_8: 0.0684 - dense_4_acc_9: 0.041 - ETA: 5s - loss: 21.1386 - dense_4_loss: 2.7559 - dense_4_acc: 0.0012 - dense_4_acc_1: 0.3891 - dense_4_acc_2: 0.1428 - dense_4_acc_3: 0.0527 - dense_4_acc_4: 0.9222 - dense_4_acc_5: 0.0577 - dense_4_acc_6: 0.0052 - dense_4_acc_7: 0.9272 - dense_4_acc_8: 0.0677 - dense_4_acc_9: 0.042 - ETA: 4s - loss: 21.1106 - dense_4_loss: 2.7529 - dense_4_acc: 0.0012 - dense_4_acc_1: 0.3917 - dense_4_acc_2: 0.1441 - dense_4_acc_3: 0.0539 - dense_4_acc_4: 0.9231 - dense_4_acc_5: 0.0570 - dense_4_acc_6: 0.0052 - dense_4_acc_7: 0.9281 - dense_4_acc_8: 0.0675 - dense_4_acc_9: 0.044 - ETA: 4s - loss: 21.0824 - dense_4_loss: 2.7500 - dense_4_acc: 0.0012 - dense_4_acc_1: 0.3950 - dense_4_acc_2: 0.1448 - dense_4_acc_3: 0.0544 - dense_4_acc_4: 0.9240 - dense_4_acc_5: 0.0563 - dense_4_acc_6: 0.0051 - dense_4_acc_7: 0.9289 - dense_4_acc_8: 0.0710 - dense_4_acc_9: 0.046 - ETA: 4s - loss: 21.0560 - dense_4_loss: 2.7488 - dense_4_acc: 0.0012 - dense_4_acc_1: 0.3974 - dense_4_acc_2: 0.1451 - dense_4_acc_3: 0.0551 - dense_4_acc_4: 0.9249 - dense_4_acc_5: 0.0556 - dense_4_acc_6: 0.0051 - dense_4_acc_7: 0.9298 - dense_4_acc_8: 0.0767 - dense_4_acc_9: 0.047 - ETA: 3s - loss: 21.0309 - dense_4_loss: 2.7490 - dense_4_acc: 0.0012 - dense_4_acc_1: 0.4003 - dense_4_acc_2: 0.1465 - dense_4_acc_3: 0.0547 - dense_4_acc_4: 0.9258 - dense_4_acc_5: 0.0550 - dense_4_acc_6: 0.0050 - dense_4_acc_7: 0.9306 - dense_4_acc_8: 0.0803 - dense_4_acc_9: 0.048 - ETA: 3s - loss: 21.0050 - dense_4_loss: 2.7460 - dense_4_acc: 0.0013 - dense_4_acc_1: 0.4029 - dense_4_acc_2: 0.1472 - dense_4_acc_3: 0.0545 - dense_4_acc_4: 0.9267 - dense_4_acc_5: 0.0544 - dense_4_acc_6: 0.0049 - dense_4_acc_7: 0.9314 - dense_4_acc_8: 0.0814 - dense_4_acc_9: 0.0500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - ETA: 3s - loss: 20.9835 - dense_4_loss: 2.7453 - dense_4_acc: 0.0045 - dense_4_acc_1: 0.4052 - dense_4_acc_2: 0.1473 - dense_4_acc_3: 0.0539 - dense_4_acc_4: 0.9275 - dense_4_acc_5: 0.0537 - dense_4_acc_6: 0.0049 - dense_4_acc_7: 0.9322 - dense_4_acc_8: 0.0813 - dense_4_acc_9: 0.050 - ETA: 3s - loss: 20.9582 - dense_4_loss: 2.7431 - dense_4_acc: 0.0113 - dense_4_acc_1: 0.4075 - dense_4_acc_2: 0.1472 - dense_4_acc_3: 0.0533 - dense_4_acc_4: 0.9283 - dense_4_acc_5: 0.0531 - dense_4_acc_6: 0.0048 - dense_4_acc_7: 0.9329 - dense_4_acc_8: 0.0826 - dense_4_acc_9: 0.051 - ETA: 2s - loss: 20.9306 - dense_4_loss: 2.7418 - dense_4_acc: 0.0186 - dense_4_acc_1: 0.4103 - dense_4_acc_2: 0.1487 - dense_4_acc_3: 0.0527 - dense_4_acc_4: 0.9291 - dense_4_acc_5: 0.0526 - dense_4_acc_6: 0.0048 - dense_4_acc_7: 0.9337 - dense_4_acc_8: 0.0860 - dense_4_acc_9: 0.052 - ETA: 2s - loss: 20.9023 - dense_4_loss: 2.7392 - dense_4_acc: 0.0257 - dense_4_acc_1: 0.4132 - dense_4_acc_2: 0.1493 - dense_4_acc_3: 0.0521 - dense_4_acc_4: 0.9299 - dense_4_acc_5: 0.0520 - dense_4_acc_6: 0.0047 - dense_4_acc_7: 0.9344 - dense_4_acc_8: 0.0907 - dense_4_acc_9: 0.053 - ETA: 2s - loss: 20.8758 - dense_4_loss: 2.7378 - dense_4_acc: 0.0325 - dense_4_acc_1: 0.4158 - dense_4_acc_2: 0.1502 - dense_4_acc_3: 0.0515 - dense_4_acc_4: 0.9307 - dense_4_acc_5: 0.0514 - dense_4_acc_6: 0.0047 - dense_4_acc_7: 0.9351 - dense_4_acc_8: 0.0937 - dense_4_acc_9: 0.054 - ETA: 1s - loss: 20.8480 - dense_4_loss: 2.7357 - dense_4_acc: 0.0387 - dense_4_acc_1: 0.4178 - dense_4_acc_2: 0.1510 - dense_4_acc_3: 0.0518 - dense_4_acc_4: 0.9314 - dense_4_acc_5: 0.0509 - dense_4_acc_6: 0.0046 - dense_4_acc_7: 0.9358 - dense_4_acc_8: 0.0943 - dense_4_acc_9: 0.055 - ETA: 1s - loss: 20.8252 - dense_4_loss: 2.7339 - dense_4_acc: 0.0446 - dense_4_acc_1: 0.4197 - dense_4_acc_2: 0.1519 - dense_4_acc_3: 0.0521 - dense_4_acc_4: 0.9321 - dense_4_acc_5: 0.0503 - dense_4_acc_6: 0.0046 - dense_4_acc_7: 0.9365 - dense_4_acc_8: 0.0946 - dense_4_acc_9: 0.055 - ETA: 1s - loss: 20.7979 - dense_4_loss: 2.7321 - dense_4_acc: 0.0508 - dense_4_acc_1: 0.4220 - dense_4_acc_2: 0.1519 - dense_4_acc_3: 0.0525 - dense_4_acc_4: 0.9328 - dense_4_acc_5: 0.0498 - dense_4_acc_6: 0.0045 - dense_4_acc_7: 0.9372 - dense_4_acc_8: 0.0981 - dense_4_acc_9: 0.056 - ETA: 1s - loss: 20.7721 - dense_4_loss: 2.7305 - dense_4_acc: 0.0568 - dense_4_acc_1: 0.4241 - dense_4_acc_2: 0.1528 - dense_4_acc_3: 0.0522 - dense_4_acc_4: 0.9335 - dense_4_acc_5: 0.0493 - dense_4_acc_6: 0.0045 - dense_4_acc_7: 0.9378 - dense_4_acc_8: 0.1018 - dense_4_acc_9: 0.057 - ETA: 0s - loss: 20.7452 - dense_4_loss: 2.7281 - dense_4_acc: 0.0629 - dense_4_acc_1: 0.4264 - dense_4_acc_2: 0.1530 - dense_4_acc_3: 0.0516 - dense_4_acc_4: 0.9342 - dense_4_acc_5: 0.0488 - dense_4_acc_6: 0.0044 - dense_4_acc_7: 0.9385 - dense_4_acc_8: 0.1048 - dense_4_acc_9: 0.058 - ETA: 0s - loss: 20.7201 - dense_4_loss: 2.7272 - dense_4_acc: 0.0679 - dense_4_acc_1: 0.4277 - dense_4_acc_2: 0.1540 - dense_4_acc_3: 0.0511 - dense_4_acc_4: 0.9349 - dense_4_acc_5: 0.0483 - dense_4_acc_6: 0.0044 - dense_4_acc_7: 0.9391 - dense_4_acc_8: 0.1065 - dense_4_acc_9: 0.058 - ETA: 0s - loss: 20.6946 - dense_4_loss: 2.7260 - dense_4_acc: 0.0733 - dense_4_acc_1: 0.4295 - dense_4_acc_2: 0.1543 - dense_4_acc_3: 0.0506 - dense_4_acc_4: 0.9356 - dense_4_acc_5: 0.0478 - dense_4_acc_6: 0.0043 - dense_4_acc_7: 0.9397 - dense_4_acc_8: 0.1073 - dense_4_acc_9: 0.059 - 26s 3ms/step - loss: 20.6682 - dense_4_loss: 2.7241 - dense_4_acc: 0.0789 - dense_4_acc_1: 0.4315 - dense_4_acc_2: 0.1548 - dense_4_acc_3: 0.0503 - dense_4_acc_4: 0.9362 - dense_4_acc_5: 0.0473 - dense_4_acc_6: 0.0043 - dense_4_acc_7: 0.9403 - dense_4_acc_8: 0.1079 - dense_4_acc_9: 0.0609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ffd1032fd0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-03\n",
      "source: 5 April 09\n",
      "output: 2009-05-05\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-21\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09\n",
      "source: March 3 2001\n",
      "output: 2001-03-03\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    \n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "    source=source[np.newaxis, :]\n",
    "\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 30, 37)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 30, 64)       17920       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm_6[0][0]                     \n",
      "                                                                 lstm_6[1][0]                     \n",
      "                                                                 lstm_6[2][0]                     \n",
      "                                                                 lstm_6[3][0]                     \n",
      "                                                                 lstm_6[4][0]                     \n",
      "                                                                 lstm_6[5][0]                     \n",
      "                                                                 lstm_6[6][0]                     \n",
      "                                                                 lstm_6[7][0]                     \n",
      "                                                                 lstm_6[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[40][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[41][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[42][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[43][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[44][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[45][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[46][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[47][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[48][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 repeat_vector_1[49][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[40][0]             \n",
      "                                                                 concatenate_1[41][0]             \n",
      "                                                                 concatenate_1[42][0]             \n",
      "                                                                 concatenate_1[43][0]             \n",
      "                                                                 concatenate_1[44][0]             \n",
      "                                                                 concatenate_1[45][0]             \n",
      "                                                                 concatenate_1[46][0]             \n",
      "                                                                 concatenate_1[47][0]             \n",
      "                                                                 concatenate_1[48][0]             \n",
      "                                                                 concatenate_1[49][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 30, 1)        11          dense_1[40][0]                   \n",
      "                                                                 dense_1[41][0]                   \n",
      "                                                                 dense_1[42][0]                   \n",
      "                                                                 dense_1[43][0]                   \n",
      "                                                                 dense_1[44][0]                   \n",
      "                                                                 dense_1[45][0]                   \n",
      "                                                                 dense_1[46][0]                   \n",
      "                                                                 dense_1[47][0]                   \n",
      "                                                                 dense_1[48][0]                   \n",
      "                                                                 dense_1[49][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_2[40][0]                   \n",
      "                                                                 dense_2[41][0]                   \n",
      "                                                                 dense_2[42][0]                   \n",
      "                                                                 dense_2[43][0]                   \n",
      "                                                                 dense_2[44][0]                   \n",
      "                                                                 dense_2[45][0]                   \n",
      "                                                                 dense_2[46][0]                   \n",
      "                                                                 dense_2[47][0]                   \n",
      "                                                                 dense_2[48][0]                   \n",
      "                                                                 dense_2[49][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[40][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[41][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[42][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[43][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[44][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[45][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[46][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[47][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[48][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 attention_weights[49][0]         \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 64), (None,  33024       dot_1[40][0]                     \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[41][0]                     \n",
      "                                                                 lstm_6[0][0]                     \n",
      "                                                                 lstm_6[0][2]                     \n",
      "                                                                 dot_1[42][0]                     \n",
      "                                                                 lstm_6[1][0]                     \n",
      "                                                                 lstm_6[1][2]                     \n",
      "                                                                 dot_1[43][0]                     \n",
      "                                                                 lstm_6[2][0]                     \n",
      "                                                                 lstm_6[2][2]                     \n",
      "                                                                 dot_1[44][0]                     \n",
      "                                                                 lstm_6[3][0]                     \n",
      "                                                                 lstm_6[3][2]                     \n",
      "                                                                 dot_1[45][0]                     \n",
      "                                                                 lstm_6[4][0]                     \n",
      "                                                                 lstm_6[4][2]                     \n",
      "                                                                 dot_1[46][0]                     \n",
      "                                                                 lstm_6[5][0]                     \n",
      "                                                                 lstm_6[5][2]                     \n",
      "                                                                 dot_1[47][0]                     \n",
      "                                                                 lstm_6[6][0]                     \n",
      "                                                                 lstm_6[6][2]                     \n",
      "                                                                 dot_1[48][0]                     \n",
      "                                                                 lstm_6[7][0]                     \n",
      "                                                                 lstm_6[7][2]                     \n",
      "                                                                 dot_1[49][0]                     \n",
      "                                                                 lstm_6[8][0]                     \n",
      "                                                                 lstm_6[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 11)           715         lstm_6[0][0]                     \n",
      "                                                                 lstm_6[1][0]                     \n",
      "                                                                 lstm_6[2][0]                     \n",
      "                                                                 lstm_6[3][0]                     \n",
      "                                                                 lstm_6[4][0]                     \n",
      "                                                                 lstm_6[5][0]                     \n",
      "                                                                 lstm_6[6][0]                     \n",
      "                                                                 lstm_6[7][0]                     \n",
      "                                                                 lstm_6[8][0]                     \n",
      "                                                                 lstm_6[9][0]                     \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'c0' with dtype float and shape [?,64]\n\t [[Node: c0 = Placeholder[dtype=DT_FLOAT, shape=[?,64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-5eda3607d2b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mattention_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot_attention_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhuman_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minv_machine_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Tuesday 09 Oct 1993\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Senior\\DeepLearning.ai\\5 Sequence Models\\week5.3_Sequence model and Attention mechanism\\Machine Translation\\nmt_utils.py\u001b[0m in \u001b[0;36mplot_attention_map\u001b[1;34m(model, input_vocabulary, inv_output_vocabulary, text, n_s, num, Tx, Ty)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_output_at\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'c0' with dtype float and shape [?,64]\n\t [[Node: c0 = Placeholder[dtype=DT_FLOAT, shape=[?,64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday 09 Oct 1993\", num = 7, n_s = 64)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "n16CQ",
   "launcher_item_id": "npjGi"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
